{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "# MIT License\n",
    "\n",
    "# Copyright (c) 2024 Hoel Kervadec\n",
    "\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "# of this software and associated documentation files (the \"Software\"), to deal\n",
    "# in the Software without restriction, including without limitation the rights\n",
    "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "# copies of the Software, and to permit persons to whom the Software is\n",
    "# furnished to do so, subject to the following conditions:\n",
    "\n",
    "# The above copyright notice and this permission notice shall be included in all\n",
    "# copies or substantial portions of the Software.\n",
    "\n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "# SOFTWARE.\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Callable, Union\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.io import read_image, ImageReadMode\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "\n",
    "def make_dataset(root, subset: str) -> list[tuple[Path, Path]]:\n",
    "    assert subset in [\"train\", \"val\", \"test\"]\n",
    "\n",
    "    root = Path(root)\n",
    "\n",
    "    img_path = root / subset / \"img\"\n",
    "    full_path = root / subset / \"gt\"\n",
    "\n",
    "    images = sorted(img_path.glob(\"*.png\"))\n",
    "    full_labels = sorted(full_path.glob(\"*.png\"))\n",
    "\n",
    "    return list(zip(images, full_labels))\n",
    "\n",
    "def make_volume_dataset(root, subset: str) -> list[tuple[Path, Path]]:\n",
    "    assert subset in [\"train\", \"val\", \"test\"]\n",
    "\n",
    "    root = Path(root)\n",
    "\n",
    "    img_path = root / subset / \"img\"\n",
    "    full_path = root / subset / \"gt\"\n",
    "\n",
    "    images = sorted(img_path.glob(\"*.png\"))\n",
    "    full_labels = sorted(full_path.glob(\"*.png\"))\n",
    "    # till here same as above    \n",
    "\n",
    "    \n",
    "    # get all slices for a volume\n",
    "    volumes = [[]]\n",
    "    labels = [[]]\n",
    "    \n",
    "    patient_id = int(images[0].stem.split(\"_\")[1])\n",
    "    for image, label in zip(images, full_labels):\n",
    "        if patient_id == int(image.stem.split(\"_\")[1]):\n",
    "            assert patient_id == int(label.stem.split(\"_\")[1]), f\"Patient ID mismatch: {image.stem} != {label.stem}\"\n",
    "            volumes[-1].append(image)\n",
    "            labels[-1].append(label)\n",
    "        else:\n",
    "            patient_id = int(image.stem.split(\"_\")[1])\n",
    "            assert patient_id == int(label.stem.split(\"_\")[1]), f\"Patient ID mismatch: {image.stem} != {label.stem}\"\n",
    "            volumes.append([image])\n",
    "            labels.append([label])\n",
    "        \n",
    "    return list(zip(volumes, labels))\n",
    "        \n",
    "\n",
    "class SliceDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        subset,\n",
    "        root_dir: Path,\n",
    "        img_transform: Callable = None,\n",
    "        gt_transform: Callable = None,\n",
    "        augment: bool = False,\n",
    "        equalize: bool = False,\n",
    "        debug: bool = False,\n",
    "    ):\n",
    "        self.root_dir: Path = root_dir\n",
    "        self.img_transform: Callable = img_transform\n",
    "        self.gt_transform: Callable = gt_transform\n",
    "        self.augmentation: bool = augment\n",
    "        self.equalize: bool = equalize\n",
    "\n",
    "        self.files = make_dataset(root_dir, subset)\n",
    "        if debug:\n",
    "            self.files = self.files[:10]\n",
    "\n",
    "        subset = f\"'{subset.capitalize()}'\"\n",
    "        print(f\">> Created {subset:<7} dataset with {len(self)} images!\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, index) -> dict[str, Union[Tensor, int, str]]:\n",
    "        img_path, gt_path = self.files[index]\n",
    "\n",
    "        img: Tensor = self.img_transform(\n",
    "            read_image(str(img_path), mode=ImageReadMode.GRAY)\n",
    "        )\n",
    "        gt: Tensor = self.gt_transform(\n",
    "            read_image(str(gt_path), mode=ImageReadMode.GRAY)\n",
    "        )\n",
    "\n",
    "        # img: Tensor = self.img_transform(Image.open(str(img_path)))\n",
    "        # gt: Tensor = self.gt_transform(Image.open(str(gt_path)))\n",
    "\n",
    "        print(img.shape, gt.shape)\n",
    "        _, W, H = img.shape\n",
    "        K, _, _ = gt.shape\n",
    "        # / assert gt.shape == (K, W, H)\n",
    "\n",
    "        return {\"images\": img, \"gts\": gt, \"stems\": img_path.stem, \"shape\": (K, W, H)}\n",
    "    \n",
    "    \n",
    "class VolumeDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        subset,\n",
    "        root_dir: Path,\n",
    "        img_transform: Callable = None,\n",
    "        gt_transform: Callable = None,\n",
    "        augment: bool = False,\n",
    "        equalize: bool = False,\n",
    "        debug: bool = False,\n",
    "        num_workers: int = 6,  # Add num_workers parameter\n",
    "    ):\n",
    "        self.root_dir: Path = root_dir\n",
    "        self.img_transform: Callable = img_transform\n",
    "        self.gt_transform: Callable = gt_transform\n",
    "        self.augmentation: bool = augment\n",
    "        self.equalize: bool = equalize\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "        self.files = make_volume_dataset(root_dir, subset)\n",
    "        if debug:\n",
    "            self.files = self.files[:10]\n",
    "\n",
    "        subset = f\"'{subset.capitalize()}'\"\n",
    "        print(f\">> Created {subset:<7} dataset with {len(self)} volumes!\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def _load_slice(self, img_gt_paths: tuple[Path, Path]):\n",
    "        \"\"\"Helper function to load a single slice image and ground truth.\"\"\"\n",
    "        img_path, gt_path = img_gt_paths\n",
    "        img = self.img_transform(\n",
    "            read_image(str(img_path), mode=ImageReadMode.GRAY)\n",
    "        )\n",
    "        gt = self.gt_transform(\n",
    "            read_image(str(gt_path), mode=ImageReadMode.GRAY)\n",
    "        )\n",
    "        return img, gt\n",
    "\n",
    "    def __getitem__(self, index) -> dict[str, Union[torch.Tensor, int, str]]:\n",
    "        img_paths, gt_paths = self.files[index]\n",
    "\n",
    "        # Use a ThreadPoolExecutor to load slices in parallel\n",
    "        with ThreadPoolExecutor(max_workers=self.num_workers) as executor:\n",
    "            results = list(executor.map(self._load_slice, zip(img_paths, gt_paths)))\n",
    "\n",
    "        # Reconstruct the volume and ground truth from slices\n",
    "        volume = torch.cat([img for img, _ in results], dim=0)\n",
    "        volume_gt = torch.cat([gt for _, gt in results], dim=0)\n",
    "\n",
    "        D, W, H = volume.shape\n",
    "        K, _, _ = volume_gt.shape\n",
    "        assert volume_gt.shape == (K, W, H), \"Mismatch in ground truth and volume shape\"\n",
    "\n",
    "        return {\"volume\": volume, \"gt\": volume_gt, \"shape\": (D, W, H)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Arguments:\n",
      "model_name         : ENet                                               ; include_background : False                                              \n",
      "mode               : full                                               ; seed               : 42                                                 \n",
      "dataset            : SEGTHOR                                            ; precision          : 32                                                 \n",
      "data_dir           : /scratch-shared/scur2480/data                      ; cpu                : False                                              \n",
      "train_3d           : True                                               ; num_workers        : 18                                                 \n",
      "epochs             : 50                                                 ; dest               : results/SEGTHOR/full/ENet/test_lightning_dicefocal2\n",
      "batch_size         : 100                                                ; debug              : False                                              \n",
      "temperature        : 1                                                  ; wandb_project_name : test_lightning_with_enet                           \n",
      "lr                 : 0.0005                                             ; model              : <class 'models.ENet.ENet'>                         \n",
      "loss               : dicefocal                                          \n",
      "\n",
      "Seed set to 42\n",
      ">>> Setting up to train on SEGTHOR with full\n",
      ">> Created 'Train' dataset with 30 volumes!\n",
      ">> Created 'Val'   dataset with 10 volumes!\n",
      ">> Exporting one sample from train loader...\n"
     ]
    }
   ],
   "source": [
    "!python main.py --model_name ENet --dataset SEGTHOR --data_dir /scratch-shared/scur2480/data \\\n",
    " --epoch 50 --batch_size 100 --num_workers 18 --wandb_project_name test_lightning_with_enet \\\n",
    " --loss dicefocal --dest results/SEGTHOR/full/ENet/test_lightning_dicefocal2 --train_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import v2\n",
    "K = 5\n",
    "root_dir = \"/scratch-shared/scur2480/data/SEGTHOR\"\n",
    "batch_size = 2\n",
    "class ReScale(v2.Transform):\n",
    "    def __init__(self, K):\n",
    "        self.scale = 1 / (255 / (K - 1)) if K != 5 else 1 / 63\n",
    "\n",
    "    def __call__(self, img):\n",
    "        return img * self.scale\n",
    "\n",
    "\n",
    "class Class2OneHot(v2.Transform):\n",
    "    def __init__(self, K):\n",
    "        self.K = K\n",
    "\n",
    "    def __call__(self, seg):\n",
    "        b, *img_shape = seg.shape\n",
    "\n",
    "        device = seg.device\n",
    "        res = torch.zeros(\n",
    "            (b, self.K, *img_shape), dtype=torch.int32, device=device\n",
    "        ).scatter_(1, seg[:, None, ...], 1)\n",
    "        return res[0]\n",
    "# Transforms\n",
    "img_transform = v2.Compose([v2.ToDtype(torch.float32, scale=True)])\n",
    "gt_transform = v2.Compose([ReScale(K), v2.ToDtype(torch.int64), Class2OneHot(K)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Created 'Train' dataset with 30 volumes!\n"
     ]
    }
   ],
   "source": [
    "d = VolumeDataset(\"train\", root_dir, img_transform, gt_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([147, 256, 256])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.__getitem__(0)['volume'].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai4mi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N2KJF9idDJXS",
    "outputId": "8d9c42e4-df83-4521-9aa3-660c266c3259"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'ai4mi_project'...\n",
      "remote: Enumerating objects: 967, done.\u001b[K\n",
      "remote: Counting objects: 100% (169/169), done.\u001b[K\n",
      "remote: Compressing objects: 100% (80/80), done.\u001b[K\n",
      "remote: Total 967 (delta 109), reused 110 (delta 87), pack-reused 798 (from 1)\u001b[K\n",
      "Receiving objects: 100% (967/967), 37.65 MiB | 11.44 MiB/s, done.\n",
      "Resolving deltas: 100% (482/482), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone --branch architecture https://github.com/danilotpnta/ai4mi_project.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N82LtVqUEK7s"
   },
   "outputs": [],
   "source": [
    "!cd nnUNet/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "t26QYg-TEgTp"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"/content/ai4mi_project/models/nnUNet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "XBmDT8gHEOzq",
    "outputId": "fa7c11e4-6a5a-4827-d121-7ab0a22757a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///content/ai4mi_project/models/nnUNet\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: torch>=2.1.2 in /usr/local/lib/python3.10/dist-packages (from nnunetv2==2.5.1) (2.4.1+cu121)\n",
      "Collecting acvl-utils<0.3,>=0.2 (from nnunetv2==2.5.1)\n",
      "  Downloading acvl_utils-0.2.tar.gz (18 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting dynamic-network-architectures<0.4,>=0.3.1 (from nnunetv2==2.5.1)\n",
      "  Downloading dynamic_network_architectures-0.3.1.tar.gz (20 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nnunetv2==2.5.1) (4.66.5)\n",
      "Collecting dicom2nifti (from nnunetv2==2.5.1)\n",
      "  Downloading dicom2nifti-2.5.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from nnunetv2==2.5.1) (1.13.1)\n",
      "Collecting batchgenerators>=0.25 (from nnunetv2==2.5.1)\n",
      "  Downloading batchgenerators-0.25.tar.gz (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.7/61.7 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.10/dist-packages (from nnunetv2==2.5.1) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from nnunetv2==2.5.1) (1.5.2)\n",
      "Requirement already satisfied: scikit-image>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from nnunetv2==2.5.1) (0.24.0)\n",
      "Collecting SimpleITK>=2.2.1 (from nnunetv2==2.5.1)\n",
      "  Downloading SimpleITK-2.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from nnunetv2==2.5.1) (2.2.2)\n",
      "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from nnunetv2==2.5.1) (0.20.3)\n",
      "Requirement already satisfied: tifffile in /usr/local/lib/python3.10/dist-packages (from nnunetv2==2.5.1) (2024.9.20)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from nnunetv2==2.5.1) (2.32.3)\n",
      "Requirement already satisfied: nibabel in /usr/local/lib/python3.10/dist-packages (from nnunetv2==2.5.1) (5.2.1)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from nnunetv2==2.5.1) (3.7.1)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (from nnunetv2==2.5.1) (0.13.2)\n",
      "Collecting imagecodecs (from nnunetv2==2.5.1)\n",
      "  Downloading imagecodecs-2024.9.22-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Collecting yacs (from nnunetv2==2.5.1)\n",
      "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
      "Collecting batchgeneratorsv2>=0.2 (from nnunetv2==2.5.1)\n",
      "  Downloading batchgeneratorsv2-0.2.1.tar.gz (34 kB)\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from nnunetv2==2.5.1) (0.8.0)\n",
      "Collecting connected-components-3d (from acvl-utils<0.3,>=0.2->nnunetv2==2.5.1)\n",
      "  Downloading connected_components_3d-3.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (32 kB)\n",
      "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from batchgenerators>=0.25->nnunetv2==2.5.1) (10.4.0)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from batchgenerators>=0.25->nnunetv2==2.5.1) (1.0.0)\n",
      "Collecting unittest2 (from batchgenerators>=0.25->nnunetv2==2.5.1)\n",
      "  Downloading unittest2-1.1.0-py2.py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: threadpoolctl in /usr/local/lib/python3.10/dist-packages (from batchgenerators>=0.25->nnunetv2==2.5.1) (3.5.0)\n",
      "Collecting fft-conv-pytorch (from batchgeneratorsv2>=0.2->nnunetv2==2.5.1)\n",
      "  Downloading fft_conv_pytorch-1.2.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: networkx>=2.8 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.19.3->nnunetv2==2.5.1) (3.3)\n",
      "Requirement already satisfied: imageio>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.19.3->nnunetv2==2.5.1) (2.35.1)\n",
      "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.19.3->nnunetv2==2.5.1) (24.1)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.19.3->nnunetv2==2.5.1) (0.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.2->nnunetv2==2.5.1) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.2->nnunetv2==2.5.1) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.2->nnunetv2==2.5.1) (1.13.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.2->nnunetv2==2.5.1) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.2->nnunetv2==2.5.1) (2024.6.1)\n",
      "Collecting pydicom>=2.2.0 (from dicom2nifti->nnunetv2==2.5.1)\n",
      "  Downloading pydicom-3.0.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Collecting python-gdcm (from dicom2nifti->nnunetv2==2.5.1)\n",
      "  Downloading python_gdcm-3.0.24.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->nnunetv2==2.5.1) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->nnunetv2==2.5.1) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->nnunetv2==2.5.1) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->nnunetv2==2.5.1) (1.4.7)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->nnunetv2==2.5.1) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->nnunetv2==2.5.1) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->nnunetv2==2.5.1) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->nnunetv2==2.5.1) (2024.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->nnunetv2==2.5.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->nnunetv2==2.5.1) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->nnunetv2==2.5.1) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->nnunetv2==2.5.1) (2024.8.30)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->nnunetv2==2.5.1) (1.4.2)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from yacs->nnunetv2==2.5.1) (6.0.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->nnunetv2==2.5.1) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.1.2->nnunetv2==2.5.1) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.1.2->nnunetv2==2.5.1) (1.3.0)\n",
      "Collecting argparse (from unittest2->batchgenerators>=0.25->nnunetv2==2.5.1)\n",
      "  Downloading argparse-1.4.0-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting traceback2 (from unittest2->batchgenerators>=0.25->nnunetv2==2.5.1)\n",
      "  Downloading traceback2-1.4.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting linecache2 (from traceback2->unittest2->batchgenerators>=0.25->nnunetv2==2.5.1)\n",
      "  Downloading linecache2-1.0.0-py2.py3-none-any.whl.metadata (1000 bytes)\n",
      "Downloading SimpleITK-2.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (52.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.4/52.4 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dicom2nifti-2.5.0-py3-none-any.whl (43 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading imagecodecs-2024.9.22-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.3/43.3 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
      "Downloading pydicom-3.0.1-py3-none-any.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m81.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading connected_components_3d-3.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m66.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fft_conv_pytorch-1.2.0-py3-none-any.whl (6.8 kB)\n",
      "Downloading python_gdcm-3.0.24.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading unittest2-1.1.0-py2.py3-none-any.whl (96 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
      "Downloading traceback2-1.4.0-py2.py3-none-any.whl (16 kB)\n",
      "Downloading linecache2-1.0.0-py2.py3-none-any.whl (12 kB)\n",
      "Building wheels for collected packages: nnunetv2, acvl-utils, batchgenerators, batchgeneratorsv2, dynamic-network-architectures\n",
      "  Building editable for nnunetv2 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for nnunetv2: filename=nnunetv2-2.5.1-0.editable-py3-none-any.whl size=4898 sha256=56a85d7b68ec40a8685934df1fc4cb32539cf38c6d422f92c083a5e4e4188b27\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-6grm5vr8/wheels/d8/e7/47/877b555b089f497b3ed09ff1726d6e6f789b73150cd70d8356\n",
      "  Building wheel for acvl-utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for acvl-utils: filename=acvl_utils-0.2-py3-none-any.whl size=22438 sha256=3c7870c2e8f56af5958ce6ecd141386cf87190e3ec5fcedac6d6084c7e23faa7\n",
      "  Stored in directory: /root/.cache/pip/wheels/ad/f0/84/52e8897591e66339bd2796681b9540b6c5e453c1461fa92a9e\n",
      "  Building wheel for batchgenerators (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for batchgenerators: filename=batchgenerators-0.25-py3-none-any.whl size=89008 sha256=2d55cef5015efaf91dfa3b8d5433bbce23f107f47c57c28b42fde4330d5a93bf\n",
      "  Stored in directory: /root/.cache/pip/wheels/9e/b0/1b/40912fb58eb167b86cbc444ddb2e6ba382b248215295f932e2\n",
      "  Building wheel for batchgeneratorsv2 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for batchgeneratorsv2: filename=batchgeneratorsv2-0.2.1-py3-none-any.whl size=45184 sha256=ca2c4c5273827ed68e933c8369d6275b17721b9ea03923aba07057169ab7fc05\n",
      "  Stored in directory: /root/.cache/pip/wheels/a7/20/91/33993997db216e7b946d379850c47837d2478be49377a6cb41\n",
      "  Building wheel for dynamic-network-architectures (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for dynamic-network-architectures: filename=dynamic_network_architectures-0.3.1-py3-none-any.whl size=30049 sha256=e17b6063f0aac9c32fb1600d050a52f31fee6f4ada9dad23c9187c7f16ef0399\n",
      "  Stored in directory: /root/.cache/pip/wheels/55/1b/13/a6419c8dbf998b9343710355ec3edc5c8e24d9b7b22eec95fb\n",
      "Successfully built nnunetv2 acvl-utils batchgenerators batchgeneratorsv2 dynamic-network-architectures\n",
      "Installing collected packages: SimpleITK, linecache2, argparse, yacs, traceback2, python-gdcm, pydicom, imagecodecs, connected-components-3d, unittest2, dicom2nifti, fft-conv-pytorch, dynamic-network-architectures, batchgenerators, batchgeneratorsv2, acvl-utils, nnunetv2\n",
      "Successfully installed SimpleITK-2.4.0 acvl-utils-0.2 argparse-1.4.0 batchgenerators-0.25 batchgeneratorsv2-0.2.1 connected-components-3d-3.19.0 dicom2nifti-2.5.0 dynamic-network-architectures-0.3.1 fft-conv-pytorch-1.2.0 imagecodecs-2024.9.22 linecache2-1.0.0 nnunetv2-2.5.1 pydicom-3.0.1 python-gdcm-3.0.24.1 traceback2-1.4.0 unittest2-1.1.0 yacs-0.1.8\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "88b1b9dcb06c45378f43b33bc7882064",
       "pip_warning": {
        "packages": [
         "argparse"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5F8V9p6uFibS",
    "outputId": "d2955cbc-3244-4772-965c-f5f27afedf03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-10-14 09:58:11--  https://downloads.rclone.org/v1.56.0/rclone-v1.56.0-linux-amd64.deb\n",
      "Resolving downloads.rclone.org (downloads.rclone.org)... 95.217.6.16, 2a01:4f9:c012:7154::1\n",
      "Connecting to downloads.rclone.org (downloads.rclone.org)|95.217.6.16|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 14972456 (14M) [application/vnd.debian.binary-package]\n",
      "Saving to: ‘rclone-v1.56.0-linux-amd64.deb’\n",
      "\n",
      "rclone-v1.56.0-linu 100%[===================>]  14.28M  6.88MB/s    in 2.1s    \n",
      "\n",
      "2024-10-14 09:58:14 (6.88 MB/s) - ‘rclone-v1.56.0-linux-amd64.deb’ saved [14972456/14972456]\n",
      "\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "Note, selecting 'rclone' instead of './rclone-v1.56.0-linux-amd64.deb'\n",
      "The following NEW packages will be installed:\n",
      "  rclone\n",
      "0 upgraded, 1 newly installed, 0 to remove and 49 not upgraded.\n",
      "Need to get 0 B/15.0 MB of archives.\n",
      "After this operation, 48.1 MB of additional disk space will be used.\n",
      "Get:1 /content/ai4mi_project/models/nnUNet/rclone-v1.56.0-linux-amd64.deb rclone amd64 1.56.0 [15.0 MB]\n",
      "Selecting previously unselected package rclone.\n",
      "(Reading database ... 123621 files and directories currently installed.)\n",
      "Preparing to unpack .../rclone-v1.56.0-linux-amd64.deb ...\n",
      "Unpacking rclone (1.56.0) ...\n",
      "Setting up rclone (1.56.0) ...\n",
      "Processing triggers for man-db (2.10.2-1) ...\n"
     ]
    }
   ],
   "source": [
    "!wget https://downloads.rclone.org/v1.56.0/rclone-v1.56.0-linux-amd64.deb\n",
    "!apt install ./rclone-v1.56.0-linux-amd64.deb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wMQurlLRFvty",
    "outputId": "4c3e6921-c90c-4b9a-a30e-272232e90d8c"
   },
   "outputs": [],
   "source": [
    "!rclone config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OmX2zgKPIeNC",
    "outputId": "a4e8cf33-dd04-4dc0-937d-5a7f3c1befe0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          -1 2024-02-12 15:25:13         0 Attachments\n",
      "          -1 2024-10-13 18:58:08        14 Dataset201_SegTHOR\n",
      "          -1 2021-09-13 15:41:33         3 Marketing Mgmt WG5 - Team 6\n",
      "          -1 2021-09-18 07:16:14         2 Marketing WG5 - Case 3\n",
      "          -1 2024-10-13 17:48:46         3 nnUNet_Snellius\n"
     ]
    }
   ],
   "source": [
    "!rclone lsd onedrive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G8MuxHU3Qjgs"
   },
   "outputs": [],
   "source": [
    "# !rm -rf /content/onedrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "wJNA9hdnKA59",
    "outputId": "42342998-0f71-4211-bd50-147f31292b56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024/10/14 10:30:29 NOTICE: nnUNet_preprocessed/Dataset201_SegTHOR/nnUNetPlans_3d_fullres/Patient_38.npy: Removing partially written file on error: stream error: stream ID 503; CANCEL\n",
      "2024/10/14 10:30:31 NOTICE: nnUNet_preprocessed/Dataset201_SegTHOR/nnUNetPlans_3d_fullres/Patient_38.npz: Removing partially written file on error: stream error: stream ID 505; CANCEL\n"
     ]
    }
   ],
   "source": [
    "!rclone copy onedrive:nnUNet_Snellius /content/onedrive\n",
    "# !rclone copy onedrive:nnUNet_Snellius /content/onedrive --transfers=32 --checkers=16 --fast-list\n",
    "# !rclone copy onedrive:nnUNet_Snellius /content/onedrive --transfers=16 --checkers=8 --fast-list\n",
    "# !rclone copy onedrive:nnUNet_Snellius /content/onedrive --transfers=64 --checkers=64 --drive-chunk-size=256M --drive-pacer-burst=200 --drive-pacer-min-sleep=10ms --fast-list --progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6F5-LJ93KGi2",
    "outputId": "0c2e7761-580f-4d10-dffb-25e618e03d3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nnUNet_raw: /content/onedrive/nnUNet_raw\n",
      "nnUNet_preprocessed: /content/onedrive/nnUNet_preprocessed\n",
      "nnUNet_results: /content/onedrive/nnUNet_results\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set the nnUNet environment variables\n",
    "os.environ[\"nnUNet_raw\"] = \"/content/onedrive/nnUNet_raw\"\n",
    "os.environ[\"nnUNet_preprocessed\"] = \"/content/onedrive/nnUNet_preprocessed\"\n",
    "os.environ[\"nnUNet_results\"] = \"/content/onedrive/nnUNet_results\"\n",
    "\n",
    "# Verify the environment variables\n",
    "print(\"nnUNet_raw:\", os.getenv(\"nnUNet_raw\"))\n",
    "print(\"nnUNet_preprocessed:\", os.getenv(\"nnUNet_preprocessed\"))\n",
    "print(\"nnUNet_results:\", os.getenv(\"nnUNet_results\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kri8-zQOOAzc",
    "outputId": "24fafbe2-eadb-46d8-d59f-8f629c2e1410"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/ai4mi_project/models/nnUNet\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XNPPSHmLOput",
    "outputId": "51df75bd-8946-4f63-f093-542f9fd5dad5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wandb\n",
      "  Downloading wandb-0.18.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
      "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
      "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
      "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
      "  Downloading sentry_sdk-2.16.0-py2.py3-none-any.whl.metadata (9.8 kB)\n",
      "Collecting setproctitle (from wandb)\n",
      "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (71.0.4)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Downloading wandb-0.18.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m89.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sentry_sdk-2.16.0-py2.py3-none-any.whl (313 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.8/313.8 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, gitpython, wandb\n",
      "Successfully installed docker-pycreds-0.4.0 gitdb-4.0.11 gitpython-3.1.43 sentry-sdk-2.16.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.18.3\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a-BKJHp0cSPq",
    "outputId": "8018704a-41bc-4362-9515-1e151fa20d09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting triton\n",
      "  Downloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton) (3.16.1)\n",
      "Downloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: triton\n",
      "Successfully installed triton-3.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZMz9HLguOEoH",
    "outputId": "7e2c5335-5d77-41af-92ab-74ceb2326a1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "############################\n",
      "INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md\n",
      "############################\n",
      "\n",
      "Using device: cuda:0\n",
      "/content/nnUNet/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py:164: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.grad_scaler = GradScaler() if self.device.type == 'cuda' else None\n",
      "\n",
      "#######################################################################\n",
      "Please cite the following paper when using nnU-Net:\n",
      "Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n",
      "#######################################################################\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdanilotpnta\u001b[0m (\u001b[33mai4med\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/nnUNet/wandb/run-20241013_195831-gxe41pcn\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mpeachy-voice-51\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ai4med/nnUNet\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ai4med/nnUNet/runs/gxe41pcn\u001b[0m\n",
      "2024-10-13 19:58:32.146044: do_dummy_2d_data_aug: True\n",
      "using pin_memory on device 0\n",
      "using pin_memory on device 0\n",
      "2024-10-13 19:58:36.858610: Using torch.compile...\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "\n",
      "This is the configuration used by this training:\n",
      "Configuration name: 3d_fullres\n",
      " {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [56, 192, 192], 'median_image_size_in_voxels': [170.5, 512.0, 512.0], 'spacing': [2.5, 0.9765620231628418, 0.9765620231628418], 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[1, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [1, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True} \n",
      "\n",
      "These are the global plan.json settings:\n",
      " {'dataset_name': 'Dataset201_SegTHOR', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [2.5, 0.9765620231628418, 0.9765620231628418], 'original_median_shape_after_transp': [178, 512, 512], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 7095.0, 'mean': 20.936588287353516, 'median': 40.0, 'min': -1000.0, 'percentile_00_5': -982.0, 'percentile_99_5': 273.0, 'std': 182.5902557373047}}} \n",
      "\n",
      "2024-10-13 19:58:42.388407: unpacking dataset...\n",
      "2024-10-13 19:58:48.534762: unpacking done...\n",
      "2024-10-13 19:58:48.537553: Unable to plot network architecture: nnUNet_compile is enabled!\n",
      "2024-10-13 19:58:48.544338: \n",
      "2024-10-13 19:58:48.544950: Epoch 0\n",
      "2024-10-13 19:58:48.545930: Current learning rate: 0.01\n",
      "W1013 19:59:07.470000 132410259632128 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] xindex is not in var_ranges, defaulting to unknown range.\n",
      "W1013 19:59:07.777000 132410259632128 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] xindex is not in var_ranges, defaulting to unknown range.\n",
      "W1013 19:59:07.956000 132410259632128 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] xindex is not in var_ranges, defaulting to unknown range.\n",
      "W1013 19:59:08.094000 132410259632128 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] xindex is not in var_ranges, defaulting to unknown range.\n",
      "W1013 19:59:08.216000 132410259632128 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] xindex is not in var_ranges, defaulting to unknown range.\n",
      "W1013 19:59:19.452000 132405235013184 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] d0 is not in var_ranges, defaulting to unknown range.\n",
      "W1013 19:59:20.040000 132405235013184 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] d0 is not in var_ranges, defaulting to unknown range.\n",
      "W1013 19:59:20.406000 132405235013184 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] d0 is not in var_ranges, defaulting to unknown range.\n",
      "W1013 19:59:20.724000 132405235013184 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] d0 is not in var_ranges, defaulting to unknown range.\n",
      "W1013 19:59:22.266000 132405235013184 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] d0 is not in var_ranges, defaulting to unknown range.\n",
      "W1013 19:59:23.532000 132405235013184 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q0 is not in var_ranges, defaulting to unknown range.\n",
      "W1013 19:59:23.588000 132405235013184 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z0 is not in var_ranges, defaulting to unknown range.\n",
      "W1013 19:59:24.323000 132405235013184 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q0 is not in var_ranges, defaulting to unknown range.\n",
      "W1013 19:59:24.424000 132405235013184 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z0 is not in var_ranges, defaulting to unknown range.\n",
      "W1013 19:59:24.862000 132405235013184 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q0 is not in var_ranges, defaulting to unknown range.\n",
      "W1013 19:59:24.944000 132405235013184 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z0 is not in var_ranges, defaulting to unknown range.\n",
      "W1013 19:59:25.341000 132405235013184 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q0 is not in var_ranges, defaulting to unknown range.\n",
      "W1013 19:59:25.432000 132405235013184 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z0 is not in var_ranges, defaulting to unknown range.\n",
      "W1013 19:59:27.442000 132405235013184 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q0 is not in var_ranges, defaulting to unknown range.\n",
      "W1013 19:59:27.664000 132405235013184 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z0 is not in var_ranges, defaulting to unknown range.\n",
      "W1013 19:59:29.405000 132405235013184 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x0 is not in var_ranges, defaulting to unknown range.\n",
      "W1013 19:59:29.951000 132405235013184 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x0 is not in var_ranges, defaulting to unknown range.\n",
      "W1013 19:59:30.151000 132405235013184 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x0 is not in var_ranges, defaulting to unknown range.\n",
      "W1013 19:59:30.350000 132405235013184 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x0 is not in var_ranges, defaulting to unknown range.\n",
      "W1013 19:59:31.293000 132405235013184 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x1 is not in var_ranges, defaulting to unknown range.\n",
      "W1013 20:04:00.259000 132410259632128 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] xindex is not in var_ranges, defaulting to unknown range.\n",
      "W1013 20:04:00.384000 132410259632128 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] xindex is not in var_ranges, defaulting to unknown range.\n",
      "W1013 20:04:00.452000 132410259632128 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] xindex is not in var_ranges, defaulting to unknown range.\n",
      "W1013 20:04:00.521000 132410259632128 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] xindex is not in var_ranges, defaulting to unknown range.\n",
      "W1013 20:04:00.593000 132410259632128 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] xindex is not in var_ranges, defaulting to unknown range.\n",
      "2024-10-13 20:04:37.055998: train_loss 1.0186\n",
      "2024-10-13 20:04:37.071846: val_loss 0.9239\n",
      "2024-10-13 20:04:37.073170: Pseudo dice [0.0, 0.4909, 0.0, 0.0]\n",
      "2024-10-13 20:04:37.074675: Epoch time: 348.51 s\n",
      "2024-10-13 20:04:37.075674: Yayy! New best EMA pseudo Dice: 0.1227\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
      "2024-10-13 20:04:41.447294: \n",
      "2024-10-13 20:04:41.448028: Epoch 1\n",
      "2024-10-13 20:04:41.448734: Current learning rate: 0.00987\n",
      "2024-10-13 20:08:59.896831: train_loss 0.7647\n",
      "2024-10-13 20:08:59.899735: val_loss 0.5678\n",
      "2024-10-13 20:08:59.900705: Pseudo dice [0.0, 0.7859, 0.0001, 0.7571]\n",
      "2024-10-13 20:08:59.901571: Epoch time: 258.45 s\n",
      "2024-10-13 20:08:59.902310: Yayy! New best EMA pseudo Dice: 0.149\n",
      "2024-10-13 20:09:04.112664: \n",
      "2024-10-13 20:09:04.114017: Epoch 2\n",
      "2024-10-13 20:09:04.114977: Current learning rate: 0.00974\n",
      "2024-10-13 20:13:21.771262: train_loss 0.4486\n",
      "2024-10-13 20:13:21.787300: val_loss 0.3974\n",
      "2024-10-13 20:13:21.788219: Pseudo dice [0.5527, 0.85, 0.8299, 0.7977]\n",
      "2024-10-13 20:13:21.789229: Epoch time: 257.66 s\n",
      "2024-10-13 20:13:21.789822: Yayy! New best EMA pseudo Dice: 0.2099\n",
      "2024-10-13 20:13:29.335901: \n",
      "2024-10-13 20:13:29.339415: Epoch 3\n",
      "2024-10-13 20:13:29.340576: Current learning rate: 0.00961\n",
      "2024-10-13 20:17:47.700360: train_loss 0.3356\n",
      "2024-10-13 20:17:47.711609: val_loss 0.3183\n",
      "2024-10-13 20:17:47.712581: Pseudo dice [0.6659, 0.8508, 0.8583, 0.8513]\n",
      "2024-10-13 20:17:47.713604: Epoch time: 258.37 s\n",
      "2024-10-13 20:17:47.714345: Yayy! New best EMA pseudo Dice: 0.2696\n",
      "2024-10-13 20:17:56.828382: \n",
      "2024-10-13 20:17:56.829186: Epoch 4\n",
      "2024-10-13 20:17:56.829777: Current learning rate: 0.00948\n",
      "2024-10-13 20:22:13.255028: train_loss 0.3044\n",
      "2024-10-13 20:22:13.272309: val_loss 0.3021\n",
      "2024-10-13 20:22:13.272739: Pseudo dice [0.6885, 0.8977, 0.8657, 0.792]\n",
      "2024-10-13 20:22:13.273552: Epoch time: 256.43 s\n",
      "2024-10-13 20:22:13.273884: Yayy! New best EMA pseudo Dice: 0.3237\n",
      "2024-10-13 20:22:17.701198: \n",
      "2024-10-13 20:22:17.702661: Epoch 5\n",
      "2024-10-13 20:22:17.703909: Current learning rate: 0.00935\n",
      "2024-10-13 20:26:35.657684: train_loss 0.2595\n",
      "2024-10-13 20:26:35.668357: val_loss 0.2334\n",
      "2024-10-13 20:26:35.669348: Pseudo dice [0.7629, 0.9181, 0.8894, 0.8894]\n",
      "2024-10-13 20:26:35.670270: Epoch time: 257.96 s\n",
      "2024-10-13 20:26:35.670899: Yayy! New best EMA pseudo Dice: 0.3778\n",
      "2024-10-13 20:26:41.628907: \n",
      "2024-10-13 20:26:41.629662: Epoch 6\n",
      "2024-10-13 20:26:41.630287: Current learning rate: 0.00923\n",
      "2024-10-13 20:30:59.084162: train_loss 0.2446\n",
      "2024-10-13 20:30:59.085841: val_loss 0.2455\n",
      "2024-10-13 20:30:59.086759: Pseudo dice [0.7618, 0.8915, 0.8786, 0.9047]\n",
      "2024-10-13 20:30:59.087566: Epoch time: 257.46 s\n",
      "2024-10-13 20:30:59.088160: Yayy! New best EMA pseudo Dice: 0.426\n",
      "2024-10-13 20:31:05.922119: \n",
      "2024-10-13 20:31:05.923351: Epoch 7\n",
      "2024-10-13 20:31:05.925122: Current learning rate: 0.0091\n",
      "2024-10-13 20:35:26.488533: train_loss 0.2381\n",
      "2024-10-13 20:35:26.494783: val_loss 0.2377\n",
      "2024-10-13 20:35:26.495285: Pseudo dice [0.7872, 0.9172, 0.8967, 0.899]\n",
      "2024-10-13 20:35:26.495692: Epoch time: 260.57 s\n",
      "2024-10-13 20:35:26.496131: Yayy! New best EMA pseudo Dice: 0.4709\n",
      "2024-10-13 20:35:36.697875: \n",
      "2024-10-13 20:35:36.699418: Epoch 8\n",
      "2024-10-13 20:35:36.700358: Current learning rate: 0.00897\n",
      "2024-10-13 20:39:56.144976: train_loss 0.2293\n",
      "2024-10-13 20:39:56.151453: val_loss 0.222\n",
      "2024-10-13 20:39:56.152156: Pseudo dice [0.7641, 0.9097, 0.9049, 0.9009]\n",
      "2024-10-13 20:39:56.152937: Epoch time: 259.45 s\n",
      "2024-10-13 20:39:56.153410: Yayy! New best EMA pseudo Dice: 0.5108\n",
      "2024-10-13 20:40:05.335373: \n",
      "2024-10-13 20:40:05.336560: Epoch 9\n",
      "2024-10-13 20:40:05.337375: Current learning rate: 0.00884\n",
      "2024-10-13 20:44:23.469161: train_loss 0.2171\n",
      "2024-10-13 20:44:23.475061: val_loss 0.1966\n",
      "2024-10-13 20:44:23.475487: Pseudo dice [0.8066, 0.9305, 0.9014, 0.9105]\n",
      "2024-10-13 20:44:23.475900: Epoch time: 258.14 s\n",
      "2024-10-13 20:44:23.476211: Yayy! New best EMA pseudo Dice: 0.5484\n",
      "2024-10-13 20:44:31.550594: \n",
      "2024-10-13 20:44:31.552065: Epoch 10\n",
      "2024-10-13 20:44:31.553223: Current learning rate: 0.0087\n",
      "2024-10-13 20:48:53.241757: train_loss 0.2181\n",
      "2024-10-13 20:48:53.255653: val_loss 0.2305\n",
      "2024-10-13 20:48:53.260444: Pseudo dice [0.7926, 0.9079, 0.8834, 0.8983]\n",
      "2024-10-13 20:48:53.261139: Epoch time: 261.69 s\n",
      "2024-10-13 20:48:53.261604: Yayy! New best EMA pseudo Dice: 0.5806\n",
      "2024-10-13 20:49:01.953884: \n",
      "2024-10-13 20:49:01.954797: Epoch 11\n",
      "2024-10-13 20:49:01.955502: Current learning rate: 0.00857\n",
      "2024-10-13 20:53:20.586227: train_loss 0.186\n",
      "2024-10-13 20:53:20.604787: val_loss 0.1881\n",
      "2024-10-13 20:53:20.606437: Pseudo dice [0.8474, 0.9253, 0.8873, 0.9265]\n",
      "2024-10-13 20:53:20.608147: Epoch time: 258.63 s\n",
      "2024-10-13 20:53:20.609048: Yayy! New best EMA pseudo Dice: 0.6122\n",
      "2024-10-13 20:53:29.077210: \n",
      "2024-10-13 20:53:29.077950: Epoch 12\n",
      "2024-10-13 20:53:29.078538: Current learning rate: 0.00844\n",
      "2024-10-13 20:57:49.142794: train_loss 0.1931\n",
      "2024-10-13 20:57:49.159245: val_loss 0.1759\n",
      "2024-10-13 20:57:49.160285: Pseudo dice [0.8502, 0.9353, 0.9061, 0.9348]\n",
      "2024-10-13 20:57:49.161369: Epoch time: 260.07 s\n",
      "2024-10-13 20:57:49.162008: Yayy! New best EMA pseudo Dice: 0.6417\n",
      "2024-10-13 20:57:57.839721: \n",
      "2024-10-13 20:57:57.850433: Epoch 13\n",
      "2024-10-13 20:57:57.851613: Current learning rate: 0.00831\n",
      "2024-10-13 21:02:16.512895: train_loss 0.1885\n",
      "2024-10-13 21:02:16.527711: val_loss 0.2027\n",
      "2024-10-13 21:02:16.528254: Pseudo dice [0.8501, 0.9328, 0.8943, 0.9248]\n",
      "2024-10-13 21:02:16.528733: Epoch time: 258.68 s\n",
      "2024-10-13 21:02:16.529164: Yayy! New best EMA pseudo Dice: 0.6676\n",
      "2024-10-13 21:02:27.538270: \n",
      "2024-10-13 21:02:27.539095: Epoch 14\n",
      "2024-10-13 21:02:27.539770: Current learning rate: 0.00818\n",
      "2024-10-13 21:06:46.126970: train_loss 0.189\n",
      "2024-10-13 21:06:46.139652: val_loss 0.1627\n",
      "2024-10-13 21:06:46.140100: Pseudo dice [0.8524, 0.9384, 0.9196, 0.9256]\n",
      "2024-10-13 21:06:46.140490: Epoch time: 258.59 s\n",
      "2024-10-13 21:06:46.141033: Yayy! New best EMA pseudo Dice: 0.6917\n",
      "2024-10-13 21:06:52.785665: \n",
      "2024-10-13 21:06:52.786337: Epoch 15\n",
      "2024-10-13 21:06:52.786681: Current learning rate: 0.00805\n",
      "2024-10-13 21:11:11.567021: train_loss 0.1777\n",
      "2024-10-13 21:11:11.584535: val_loss 0.1713\n",
      "2024-10-13 21:11:11.585831: Pseudo dice [0.8436, 0.9416, 0.9184, 0.93]\n",
      "2024-10-13 21:11:11.596306: Epoch time: 258.78 s\n",
      "2024-10-13 21:11:11.597383: Yayy! New best EMA pseudo Dice: 0.7134\n",
      "2024-10-13 21:11:20.347105: \n",
      "2024-10-13 21:11:20.348447: Epoch 16\n",
      "2024-10-13 21:11:20.350117: Current learning rate: 0.00792\n",
      "2024-10-13 21:15:39.931032: train_loss 0.1754\n",
      "2024-10-13 21:15:39.952383: val_loss 0.1902\n",
      "2024-10-13 21:15:39.953353: Pseudo dice [0.8583, 0.9535, 0.9214, 0.9347]\n",
      "2024-10-13 21:15:39.954660: Epoch time: 259.59 s\n",
      "2024-10-13 21:15:39.955492: Yayy! New best EMA pseudo Dice: 0.7337\n",
      "2024-10-13 21:15:48.775718: \n",
      "2024-10-13 21:15:48.777277: Epoch 17\n",
      "2024-10-13 21:15:48.777990: Current learning rate: 0.00779\n",
      "2024-10-13 21:20:09.122907: train_loss 0.1702\n",
      "2024-10-13 21:20:09.139981: val_loss 0.1687\n",
      "2024-10-13 21:20:09.140942: Pseudo dice [0.8578, 0.944, 0.9189, 0.9306]\n",
      "2024-10-13 21:20:09.141690: Epoch time: 260.35 s\n",
      "2024-10-13 21:20:09.142484: Yayy! New best EMA pseudo Dice: 0.7516\n",
      "2024-10-13 21:20:12.599466: \n",
      "2024-10-13 21:20:12.600078: Epoch 18\n",
      "2024-10-13 21:20:12.600482: Current learning rate: 0.00765\n",
      "2024-10-13 21:24:32.618137: train_loss 0.1739\n",
      "2024-10-13 21:24:32.642699: val_loss 0.1998\n",
      "2024-10-13 21:24:32.643888: Pseudo dice [0.8656, 0.9432, 0.93, 0.9343]\n",
      "2024-10-13 21:24:32.644490: Epoch time: 260.02 s\n",
      "2024-10-13 21:24:32.644892: Yayy! New best EMA pseudo Dice: 0.7683\n",
      "2024-10-13 21:24:41.058867: \n",
      "2024-10-13 21:24:41.059633: Epoch 19\n",
      "2024-10-13 21:24:41.060190: Current learning rate: 0.00752\n",
      "2024-10-13 21:28:58.977899: train_loss 0.1703\n",
      "2024-10-13 21:28:58.999999: val_loss 0.1707\n",
      "2024-10-13 21:28:59.000876: Pseudo dice [0.8462, 0.9435, 0.904, 0.9382]\n",
      "2024-10-13 21:28:59.001737: Epoch time: 257.92 s\n",
      "2024-10-13 21:28:59.002341: Yayy! New best EMA pseudo Dice: 0.7823\n",
      "2024-10-13 21:29:02.173064: \n",
      "2024-10-13 21:29:02.174297: Epoch 20\n",
      "2024-10-13 21:29:02.175423: Current learning rate: 0.00739\n",
      "2024-10-13 21:33:23.199903: train_loss 0.1689\n",
      "2024-10-13 21:33:23.205693: val_loss 0.1881\n",
      "2024-10-13 21:33:23.206211: Pseudo dice [0.8751, 0.9456, 0.9228, 0.9372]\n",
      "2024-10-13 21:33:23.206860: Epoch time: 261.03 s\n",
      "2024-10-13 21:33:23.207412: Yayy! New best EMA pseudo Dice: 0.7961\n",
      "2024-10-13 21:33:30.488320: \n",
      "2024-10-13 21:33:30.489827: Epoch 21\n",
      "2024-10-13 21:33:30.491127: Current learning rate: 0.00725\n",
      "2024-10-13 21:37:50.967784: train_loss 0.1752\n",
      "2024-10-13 21:37:50.980303: val_loss 0.1651\n",
      "2024-10-13 21:37:50.981678: Pseudo dice [0.8817, 0.9504, 0.9167, 0.9409]\n",
      "2024-10-13 21:37:50.982930: Epoch time: 260.48 s\n",
      "2024-10-13 21:37:50.983850: Yayy! New best EMA pseudo Dice: 0.8087\n",
      "2024-10-13 21:37:59.992056: \n",
      "2024-10-13 21:37:59.993557: Epoch 22\n",
      "2024-10-13 21:37:59.994613: Current learning rate: 0.00712\n",
      "2024-10-13 21:42:20.760959: train_loss 0.1713\n",
      "2024-10-13 21:42:20.771050: val_loss 0.1554\n",
      "2024-10-13 21:42:20.772074: Pseudo dice [0.884, 0.947, 0.9194, 0.9398]\n",
      "2024-10-13 21:42:20.773038: Epoch time: 260.77 s\n",
      "2024-10-13 21:42:20.773602: Yayy! New best EMA pseudo Dice: 0.8201\n",
      "2024-10-13 21:42:32.310188: \n",
      "2024-10-13 21:42:32.311661: Epoch 23\n",
      "2024-10-13 21:42:32.312729: Current learning rate: 0.00699\n",
      "2024-10-13 21:46:49.695761: train_loss 0.1627\n",
      "2024-10-13 21:46:49.702672: val_loss 0.1435\n",
      "2024-10-13 21:46:49.703778: Pseudo dice [0.8827, 0.9499, 0.9253, 0.946]\n",
      "2024-10-13 21:46:49.704524: Epoch time: 257.39 s\n",
      "2024-10-13 21:46:49.705425: Yayy! New best EMA pseudo Dice: 0.8307\n",
      "2024-10-13 21:46:58.319092: \n",
      "2024-10-13 21:46:58.319750: Epoch 24\n",
      "2024-10-13 21:46:58.320391: Current learning rate: 0.00685\n",
      "2024-10-13 21:51:17.721749: train_loss 0.1611\n",
      "2024-10-13 21:51:17.746595: val_loss 0.1682\n",
      "2024-10-13 21:51:17.747480: Pseudo dice [0.879, 0.9546, 0.9225, 0.9297]\n",
      "2024-10-13 21:51:17.748379: Epoch time: 259.4 s\n",
      "2024-10-13 21:51:17.748945: Yayy! New best EMA pseudo Dice: 0.8398\n",
      "2024-10-13 21:51:27.670832: \n",
      "2024-10-13 21:51:27.671527: Epoch 25\n",
      "2024-10-13 21:51:27.672031: Current learning rate: 0.00672\n",
      "2024-10-13 21:55:46.620336: train_loss 0.1547\n",
      "2024-10-13 21:55:46.623008: val_loss 0.1627\n",
      "2024-10-13 21:55:46.623620: Pseudo dice [0.8967, 0.9533, 0.9293, 0.9437]\n",
      "2024-10-13 21:55:46.624049: Epoch time: 258.95 s\n",
      "2024-10-13 21:55:46.624425: Yayy! New best EMA pseudo Dice: 0.8488\n",
      "2024-10-13 21:55:51.220377: \n",
      "2024-10-13 21:55:51.224447: Epoch 26\n",
      "2024-10-13 21:55:51.225023: Current learning rate: 0.00658\n",
      "2024-10-13 22:00:09.666486: train_loss 0.1503\n",
      "2024-10-13 22:00:09.669770: val_loss 0.1554\n",
      "2024-10-13 22:00:09.671122: Pseudo dice [0.8854, 0.9556, 0.9204, 0.9448]\n",
      "2024-10-13 22:00:09.671770: Epoch time: 258.45 s\n",
      "2024-10-13 22:00:09.673163: Yayy! New best EMA pseudo Dice: 0.8566\n",
      "2024-10-13 22:00:19.092268: \n",
      "2024-10-13 22:00:19.093249: Epoch 27\n",
      "2024-10-13 22:00:19.093956: Current learning rate: 0.00645\n",
      "2024-10-13 22:04:38.213919: train_loss 0.1439\n",
      "2024-10-13 22:04:38.218075: val_loss 0.1529\n",
      "2024-10-13 22:04:38.219425: Pseudo dice [0.8806, 0.9563, 0.9281, 0.9484]\n",
      "2024-10-13 22:04:38.220397: Epoch time: 259.12 s\n",
      "2024-10-13 22:04:38.221443: Yayy! New best EMA pseudo Dice: 0.8638\n",
      "2024-10-13 22:04:47.260416: \n",
      "2024-10-13 22:04:47.261354: Epoch 28\n",
      "2024-10-13 22:04:47.261909: Current learning rate: 0.00631\n",
      "2024-10-13 22:09:07.498070: train_loss 0.153\n",
      "2024-10-13 22:09:07.523673: val_loss 0.1449\n",
      "2024-10-13 22:09:07.524620: Pseudo dice [0.8916, 0.9584, 0.936, 0.9453]\n",
      "2024-10-13 22:09:07.525662: Epoch time: 260.24 s\n",
      "2024-10-13 22:09:07.526261: Yayy! New best EMA pseudo Dice: 0.8707\n",
      "2024-10-13 22:09:16.551501: \n",
      "2024-10-13 22:09:16.554352: Epoch 29\n",
      "2024-10-13 22:09:16.555868: Current learning rate: 0.00618\n",
      "2024-10-13 22:13:35.949253: train_loss 0.1474\n",
      "2024-10-13 22:13:35.951824: val_loss 0.1633\n",
      "2024-10-13 22:13:35.952869: Pseudo dice [0.8923, 0.9579, 0.9322, 0.9482]\n",
      "2024-10-13 22:13:35.953975: Epoch time: 259.4 s\n",
      "2024-10-13 22:13:35.954523: Yayy! New best EMA pseudo Dice: 0.8769\n",
      "2024-10-13 22:13:44.841169: \n",
      "2024-10-13 22:13:44.842321: Epoch 30\n",
      "2024-10-13 22:13:44.842912: Current learning rate: 0.00604\n",
      "2024-10-13 22:18:04.175357: train_loss 0.1528\n",
      "2024-10-13 22:18:04.188046: val_loss 0.1406\n",
      "2024-10-13 22:18:04.189518: Pseudo dice [0.8941, 0.9541, 0.92, 0.9516]\n",
      "2024-10-13 22:18:04.192452: Epoch time: 259.34 s\n",
      "2024-10-13 22:18:04.194600: Yayy! New best EMA pseudo Dice: 0.8822\n",
      "2024-10-13 22:18:12.591412: \n",
      "2024-10-13 22:18:12.592298: Epoch 31\n",
      "2024-10-13 22:18:12.592888: Current learning rate: 0.00591\n",
      "2024-10-13 22:22:31.599789: train_loss 0.1498\n",
      "2024-10-13 22:22:31.617750: val_loss 0.1372\n",
      "2024-10-13 22:22:31.618667: Pseudo dice [0.8861, 0.9575, 0.9316, 0.9512]\n",
      "2024-10-13 22:22:31.619474: Epoch time: 259.01 s\n",
      "2024-10-13 22:22:31.620197: Yayy! New best EMA pseudo Dice: 0.8871\n",
      "2024-10-13 22:22:41.969247: \n",
      "2024-10-13 22:22:41.970801: Epoch 32\n",
      "2024-10-13 22:22:41.972235: Current learning rate: 0.00577\n",
      "2024-10-13 22:27:00.618333: train_loss 0.145\n",
      "2024-10-13 22:27:00.622176: val_loss 0.1686\n",
      "2024-10-13 22:27:00.623061: Pseudo dice [0.8996, 0.9603, 0.9418, 0.9474]\n",
      "2024-10-13 22:27:00.623967: Epoch time: 258.65 s\n",
      "2024-10-13 22:27:00.624586: Yayy! New best EMA pseudo Dice: 0.8922\n",
      "2024-10-13 22:27:06.943497: \n",
      "2024-10-13 22:27:06.947174: Epoch 33\n",
      "2024-10-13 22:27:06.949334: Current learning rate: 0.00563\n",
      "2024-10-13 22:31:26.511561: train_loss 0.1331\n",
      "2024-10-13 22:31:26.516620: val_loss 0.1523\n",
      "2024-10-13 22:31:26.517257: Pseudo dice [0.8956, 0.9612, 0.9337, 0.9492]\n",
      "2024-10-13 22:31:26.517847: Epoch time: 259.57 s\n",
      "2024-10-13 22:31:26.518474: Yayy! New best EMA pseudo Dice: 0.8964\n",
      "2024-10-13 22:31:30.438477: \n",
      "2024-10-13 22:31:30.439893: Epoch 34\n",
      "2024-10-13 22:31:30.440953: Current learning rate: 0.0055\n",
      "2024-10-13 22:35:51.230504: train_loss 0.1442\n",
      "2024-10-13 22:35:51.239591: val_loss 0.1726\n",
      "2024-10-13 22:35:51.240597: Pseudo dice [0.8977, 0.9613, 0.9307, 0.9515]\n",
      "2024-10-13 22:35:51.241552: Epoch time: 260.79 s\n",
      "2024-10-13 22:35:51.242357: Yayy! New best EMA pseudo Dice: 0.9003\n",
      "2024-10-13 22:35:54.584388: \n",
      "2024-10-13 22:35:54.585515: Epoch 35\n",
      "2024-10-13 22:35:54.586485: Current learning rate: 0.00536\n",
      "2024-10-13 22:40:15.159943: train_loss 0.1504\n",
      "2024-10-13 22:40:15.165852: val_loss 0.1483\n",
      "2024-10-13 22:40:15.166346: Pseudo dice [0.8979, 0.96, 0.9331, 0.9487]\n",
      "2024-10-13 22:40:15.166748: Epoch time: 260.58 s\n",
      "2024-10-13 22:40:15.167122: Yayy! New best EMA pseudo Dice: 0.9038\n",
      "2024-10-13 22:40:23.708780: \n",
      "2024-10-13 22:40:23.709558: Epoch 36\n",
      "2024-10-13 22:40:23.710217: Current learning rate: 0.00522\n",
      "2024-10-13 22:44:45.003668: train_loss 0.1481\n",
      "2024-10-13 22:44:45.005210: val_loss 0.1429\n",
      "2024-10-13 22:44:45.006179: Pseudo dice [0.8993, 0.9536, 0.9383, 0.944]\n",
      "2024-10-13 22:44:45.006943: Epoch time: 261.3 s\n",
      "2024-10-13 22:44:45.007498: Yayy! New best EMA pseudo Dice: 0.9068\n",
      "2024-10-13 22:44:55.990781: \n",
      "2024-10-13 22:44:55.991586: Epoch 37\n",
      "2024-10-13 22:44:55.993511: Current learning rate: 0.00508\n",
      "2024-10-13 22:49:14.776833: train_loss 0.1571\n",
      "2024-10-13 22:49:14.793797: val_loss 0.1518\n",
      "2024-10-13 22:49:14.794487: Pseudo dice [0.895, 0.9598, 0.9306, 0.9458]\n",
      "2024-10-13 22:49:14.795143: Epoch time: 258.78 s\n",
      "2024-10-13 22:49:14.795879: Yayy! New best EMA pseudo Dice: 0.9094\n",
      "2024-10-13 22:49:24.501676: \n",
      "2024-10-13 22:49:24.502313: Epoch 38\n",
      "2024-10-13 22:49:24.502773: Current learning rate: 0.00494\n"
     ]
    }
   ],
   "source": [
    "!nnUNetv2_train Dataset201_SegTHOR 3d_fullres all -tr nnUNetTrainerWandB_diceFocal_NoAug -device cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ppg4KHjHOF-O",
    "outputId": "11bcdfc9-6f18-464c-e6a5-156f8ca780fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "############################\n",
      "INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md\n",
      "############################\n",
      "\n",
      "Using device: cuda:0\n",
      "/content/ai4mi_project/models/nnUNet/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py:164: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.grad_scaler = GradScaler() if self.device.type == 'cuda' else None\n",
      "\n",
      "#######################################################################\n",
      "Please cite the following paper when using nnU-Net:\n",
      "Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n",
      "#######################################################################\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/ai4mi_project/models/nnUNet/wandb/run-20241014_104241-13op851f\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mhearty-sponge-52\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ai4med/nnUNet\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ai4med/nnUNet/runs/13op851f\u001b[0m\n",
      "2024-10-14 10:42:42.221820: do_dummy_2d_data_aug: False\n",
      "2024-10-14 10:42:42.222700: Using splits from existing split file: /content/onedrive/nnUNet_preprocessed/Dataset201_SegTHOR/splits_final.json\n",
      "2024-10-14 10:42:42.224209: The split file contains 5 splits.\n",
      "2024-10-14 10:42:42.224542: Desired fold for training: 0\n",
      "2024-10-14 10:42:42.224907: This split has 32 training and 8 validation cases.\n",
      "using pin_memory on device 0\n",
      "using pin_memory on device 0\n",
      "\n",
      "2024-10-14 10:42:47.187909: Using torch.compile...\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "\n",
      "This is the configuration used by this training:\n",
      "Configuration name: 2d\n",
      " {'data_identifier': 'nnUNetPlans_2d', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 12, 'patch_size': [512, 512], 'median_image_size_in_voxels': [512.0, 512.0], 'spacing': [0.9765620231628418, 0.9765620231628418], 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 8, 'features_per_stage': [32, 64, 128, 256, 512, 512, 512, 512], 'conv_op': 'torch.nn.modules.conv.Conv2d', 'kernel_sizes': [[3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3]], 'strides': [[1, 1], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm2d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True} \n",
      "\n",
      "These are the global plan.json settings:\n",
      " {'dataset_name': 'Dataset201_SegTHOR', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [2.5, 0.9765620231628418, 0.9765620231628418], 'original_median_shape_after_transp': [178, 512, 512], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 7095.0, 'mean': 20.936588287353516, 'median': 40.0, 'min': -1000.0, 'percentile_00_5': -982.0, 'percentile_99_5': 273.0, 'std': 182.5902557373047}}} \n",
      "\n",
      "2024-10-14 10:42:50.422289: unpacking dataset...\n",
      "2024-10-14 10:42:59.356205: unpacking done...\n",
      "2024-10-14 10:42:59.359259: Unable to plot network architecture: nnUNet_compile is enabled!\n",
      "2024-10-14 10:42:59.385492: \n",
      "2024-10-14 10:42:59.386115: Epoch 0\n",
      "2024-10-14 10:42:59.386853: Current learning rate: 0.01\n",
      "W1014 10:43:21.447000 135326164037632 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] xindex is not in var_ranges, defaulting to unknown range.\n",
      "W1014 10:43:21.541000 135326164037632 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] xindex is not in var_ranges, defaulting to unknown range.\n",
      "W1014 10:43:21.630000 135326164037632 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] xindex is not in var_ranges, defaulting to unknown range.\n",
      "W1014 10:43:21.717000 135326164037632 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] xindex is not in var_ranges, defaulting to unknown range.\n",
      "W1014 10:43:21.810000 135326164037632 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] xindex is not in var_ranges, defaulting to unknown range.\n",
      "W1014 10:43:47.898000 135321123542592 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] d0 is not in var_ranges, defaulting to unknown range.\n",
      "W1014 10:43:48.327000 135321123542592 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] d0 is not in var_ranges, defaulting to unknown range.\n",
      "W1014 10:43:51.027000 135321123542592 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] d0 is not in var_ranges, defaulting to unknown range.\n",
      "W1014 10:43:52.450000 135321123542592 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q0 is not in var_ranges, defaulting to unknown range.\n",
      "W1014 10:43:52.511000 135321123542592 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z0 is not in var_ranges, defaulting to unknown range.\n",
      "W1014 10:43:53.033000 135321123542592 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q0 is not in var_ranges, defaulting to unknown range.\n",
      "W1014 10:43:53.102000 135321123542592 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z0 is not in var_ranges, defaulting to unknown range.\n",
      "W1014 10:43:53.444000 135321123542592 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q0 is not in var_ranges, defaulting to unknown range.\n",
      "W1014 10:43:53.501000 135321123542592 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z0 is not in var_ranges, defaulting to unknown range.\n",
      "W1014 10:43:54.338000 135321123542592 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q0 is not in var_ranges, defaulting to unknown range.\n",
      "W1014 10:43:54.397000 135321123542592 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z0 is not in var_ranges, defaulting to unknown range.\n",
      "W1014 10:43:57.305000 135321123542592 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q0 is not in var_ranges, defaulting to unknown range.\n",
      "W1014 10:43:57.478000 135321123542592 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z0 is not in var_ranges, defaulting to unknown range.\n",
      "W1014 10:43:59.243000 135321123542592 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x0 is not in var_ranges, defaulting to unknown range.\n",
      "W1014 10:43:59.778000 135321123542592 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x0 is not in var_ranges, defaulting to unknown range.\n",
      "W1014 10:44:00.178000 135321123542592 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x0 is not in var_ranges, defaulting to unknown range.\n",
      "W1014 10:44:00.559000 135321123542592 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x0 is not in var_ranges, defaulting to unknown range.\n",
      "W1014 10:44:02.744000 135321123542592 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x0 is not in var_ranges, defaulting to unknown range.\n",
      "W1014 10:48:04.833000 135326164037632 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] xindex is not in var_ranges, defaulting to unknown range.\n",
      "W1014 10:48:05.066000 135326164037632 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] xindex is not in var_ranges, defaulting to unknown range.\n",
      "W1014 10:48:05.273000 135326164037632 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] xindex is not in var_ranges, defaulting to unknown range.\n",
      "W1014 10:48:05.478000 135326164037632 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] xindex is not in var_ranges, defaulting to unknown range.\n",
      "W1014 10:48:05.680000 135326164037632 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] xindex is not in var_ranges, defaulting to unknown range.\n",
      "2024-10-14 10:49:24.971957: train_loss 1.012\n",
      "2024-10-14 10:49:24.977065: val_loss 0.8918\n",
      "2024-10-14 10:49:24.977822: Pseudo dice [0.0, 0.5271, 0.0, 0.0]\n",
      "2024-10-14 10:49:24.978485: Epoch time: 385.59 s\n",
      "2024-10-14 10:49:24.978964: Yayy! New best EMA pseudo Dice: 0.1318\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
      "2024-10-14 10:49:28.876835: \n",
      "2024-10-14 10:49:28.878464: Epoch 1\n",
      "2024-10-14 10:49:28.879648: Current learning rate: 0.00987\n",
      "2024-10-14 10:53:00.150833: train_loss 0.781\n",
      "2024-10-14 10:53:00.154232: val_loss 0.7266\n",
      "2024-10-14 10:53:00.155269: Pseudo dice [0.0, 0.825, 0.0, 0.2815]\n",
      "2024-10-14 10:53:00.156203: Epoch time: 211.28 s\n",
      "2024-10-14 10:53:00.156881: Yayy! New best EMA pseudo Dice: 0.1463\n",
      "2024-10-14 10:53:08.189168: \n",
      "2024-10-14 10:53:08.190045: Epoch 2\n",
      "2024-10-14 10:53:08.190724: Current learning rate: 0.00974\n",
      "2024-10-14 10:56:38.803325: train_loss 0.5828\n",
      "2024-10-14 10:56:38.807387: val_loss 0.5143\n",
      "2024-10-14 10:56:38.808307: Pseudo dice [0.0, 0.8842, 0.0, 0.8255]\n",
      "2024-10-14 10:56:38.809220: Epoch time: 210.62 s\n",
      "2024-10-14 10:56:38.809820: Yayy! New best EMA pseudo Dice: 0.1744\n",
      "2024-10-14 10:56:47.654009: \n",
      "2024-10-14 10:56:47.654951: Epoch 3\n",
      "2024-10-14 10:56:47.655640: Current learning rate: 0.00961\n",
      "2024-10-14 11:00:14.928934: train_loss 0.4383\n",
      "2024-10-14 11:00:14.935285: val_loss 0.4441\n",
      "2024-10-14 11:00:14.936055: Pseudo dice [0.0, 0.8955, 0.0, 0.8549]\n",
      "2024-10-14 11:00:14.936817: Epoch time: 207.28 s\n",
      "2024-10-14 11:00:14.937431: Yayy! New best EMA pseudo Dice: 0.2007\n",
      "2024-10-14 11:00:22.548312: \n",
      "2024-10-14 11:00:22.548953: Epoch 4\n",
      "2024-10-14 11:00:22.549309: Current learning rate: 0.00948\n",
      "2024-10-14 11:03:50.117476: train_loss 0.394\n",
      "2024-10-14 11:03:50.119329: val_loss 0.4594\n",
      "2024-10-14 11:03:50.120199: Pseudo dice [0.0, 0.8621, 0.0, 0.8427]\n",
      "2024-10-14 11:03:50.121135: Epoch time: 207.57 s\n",
      "2024-10-14 11:03:50.121706: Yayy! New best EMA pseudo Dice: 0.2232\n",
      "2024-10-14 11:03:59.076206: \n",
      "2024-10-14 11:03:59.081509: Epoch 5\n",
      "2024-10-14 11:03:59.087635: Current learning rate: 0.00935\n",
      "2024-10-14 11:07:26.671872: train_loss 0.3782\n",
      "2024-10-14 11:07:26.678075: val_loss 0.4118\n",
      "2024-10-14 11:07:26.678669: Pseudo dice [0.0, 0.8987, 0.0, 0.8743]\n",
      "2024-10-14 11:07:26.679220: Epoch time: 207.6 s\n",
      "2024-10-14 11:07:26.679724: Yayy! New best EMA pseudo Dice: 0.2452\n",
      "2024-10-14 11:07:30.191722: \n",
      "2024-10-14 11:07:30.192414: Epoch 6\n",
      "2024-10-14 11:07:30.193042: Current learning rate: 0.00923\n"
     ]
    }
   ],
   "source": [
    "!nnUNetv2_train Dataset201_SegTHOR 2d 0 -tr nnUNetTrainerWandB_diceFocal_NoAug -device cuda"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
